{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Rock Climbing Logbook (8a.nu) Analysis\n",
    "## or: Does Being Tall *Really* Help? <br>\n",
    "Steve Bachmeier <br>\n",
    "2019-03-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Code to run for report\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# Run the following to hide the In[] and Out[] margin. \n",
    "# Doing so will not allow headings to be collapsed.\n",
    "'''\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML('<style>.prompt{width: 0px; min-width: 0px; visibility: collapse}</style>'))\n",
    "'''\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# Run the following to import required libraries\n",
    "import dill\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# Run the following to load required pickled objects\n",
    "\n",
    "df_all_head = dill.load(open(\"df_all_head.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rock climbing is a tough sport. It takes physical strength, extreme endurance in strange muscle and tendon groups that are not typically targeted (finger ligaments, forearms, etc), dancer-like body awareness and balance (although I am failing hard in this arena!), and serious mental grit. With such obstacles to overcome, then, nothing is more infuriating than when I finally - finally! - successfully complete a climb without falling only to hear such grumblings as: \"Ugh, it's so easy if you're tall\" or \"It's not fair - you can just reach past the difficult holds!\"\n",
    "\n",
    "It's true that with a height of six feet and a +2 so-called ape index (that is, my tip-to-tip arm span is my height plus two inches, ie 6'2\") I can sometimes do in one big move what a shorter guy or gal might require two or three. But conversely, and I've argued this adamently for years, there are plenty of times where the next hold is perfectly at arms length for a short person whereas for me it's relatively lower and requires me to scrunch up into all sorts of weird inefficient positions. The more I crunch, the more my butt sticks out, and the farther from the wall my center of gravity goes - it's all physics! That's not even to mention the math behind leverage (where my longer arms are not necessarily an advantage) or the fact that taller people also tend to be heavier. \n",
    "\n",
    "So what's the deal? Is it better for climbing to be tall or short? And while we are at it, what about other factors like weight, gender, and years of experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data was downloaded from: https://www.kaggle.com/dcohen21/8anu-climbing-logbook.\n",
    "\n",
    "The data used was scraped from an online logbook, https://beta.8a.nu/, by David Cohen (https://www.kaggle.com/dcohen21). It should be noted that the user's code to scrape the website is no longer avaialable due to DMCA takedown.\n",
    "\n",
    "Per the data description, it was collected on 2017-9-13. The data is assumed to be accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 8a.nu description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8a.nu is a rather popular online logbook used to track rock climbs completed. A user creates an account, searches for a specific climb he/she finished, and logs information about it such as its difficulty; whether it was on-sighted (climbed without falling on the first time), flashed (climbed without falling on the first time but after having watched someone else do it), or redpointed it (climbed it without falling after previously failing); how much the liked it based on a 0-3 star rating; and any other notes.\n",
    "\n",
    "The website also includes details about each climb such as the consensus difficulty grade and rating, location, and type of climb ('rope' refers to taller climbs that are typically done with a harness/rope and belayer and 'boulder' refers to shorter climbs done with no rope).\n",
    "\n",
    "Finally, user details can be optionally input including things such as gender, height, weight, the year he/she began climbing, etc.\n",
    "\n",
    "**It is important to note that climbs on 8a.nu are only logged when successfully completed without falling.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine which - if any - attributes give climbers a statistically significant advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data wrangling\n",
    "This section outlines the data formatting completed. Refer to the Jupyter Notebook analysis.ipynb for entire analysis and code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded dataset from came in the form of an SQlite database. Using the Python ```sqlite3``` package, all tables in the database were read into ```pandas``` dataframes and then saved out as csv files. All csv files were then read back into Python into one dictionary which was finally unpacked into separate raw data dataframes; there were in the end four:\n",
    "* users (62592 rows, 22 columns)\n",
    "* ascents (~4.1 million rows, 28 columns)\n",
    "* climbing methods (5 rows, 4 columns)\n",
    "* climbing grades (83 rows, 14 columns)\n",
    "\n",
    "Each row in the users dataframe corresponded to a different unqique person and included such information ask user ID, name, location, gender, height, weight, etc.\n",
    "\n",
    "The ascents dataframe is a log of all of the climbs logged on 8a.nu and includes information like the ascent ID, the user ID of that specific ascent, the method ID, date, notes, etc.\n",
    "\n",
    "The method dataframe is a small one that includes ony five rows. It's columns consist of method ID, score, shorthand, and name. Shorthand and name are redundant (lower case vs capitalized) and are simply the types of finish a climber can get (redpoint, flash, onsight, and toprope). It's unclear what exactly score is. Note that there is not option for failed attempts (finishing but having fallen at least once in the process).\n",
    "\n",
    "Finally, the grades dataframe lists all of the current climbing grade possibilities from very easy (grade ID of 0) to world class (grade ID of 82). The grade IDs align one-to-one with the French rating system (0 to 9c+/10a). Other columns are included (including the American Yosemite Decimal System grading system) but the fact that the French grades align completely with the grade IDs implies that grade ID is a good indicator of difficulty. Essentially, it is assumed that climbs become linearly difficult from a grade ID of 0 up to a grade ID of 82."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the raw data now in usable dataframes, they can be cleaned. Refer to analysis.ipynb for the code itself.\n",
    "\n",
    "In summary:\n",
    "\n",
    "1. Drop unnecessary columns from all four dataframes\n",
    "2. Check for null/empty values in all dataframes $^{[a]}$\n",
    "3. For the user dataframe:\n",
    "  1. Convert the 'sex' variable to dummy variable 'is_female' $^{[b]}$\n",
    "  2. For the birth years that exist, extract just the year.\n",
    "  3. Drop any remaining duplicate user IDs.\n",
    "4. For the ascent dataframe:\n",
    "  1. Drop any duplicate ascent IDs.\n",
    "  2. Convert user ID column to integers.\n",
    "\n",
    "------------\n",
    "$^{[a]}$ It should be noted that there is a fair amount of null values for the 'birth' feature in the users dataframe; 55.5% of people did not bother putting their birth date into the website. It was decided not to drop the column nor to impute the values; instead, filter as necessary.\n",
    "\n",
    "$^{[b]}$ The 'sex' variable had three unique values. The vast majority were 0s and 1; four users had sex=255. As I highly doubt that the 8a.nu team has implemented non-binary gender assignment functionality into the website, those four users were simply dropped from the dataset. \n",
    "\n",
    "As for the 0s and 1s, it was found that 0s were on average heavier and taller than 1s. Further, there were significantly more 0s than 1s. As most climbers are male and males tend to be taller and heavier than females, the 'sex' category was renamed as 'is_female'. \n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data merge and new feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cleaned datasets, they can now be merged into one large working dataframe. This is completed by merging the user dataframe onto the ascent dataframe using the user ID primary keys. The columns of the new working dataframe are then rearranged and two new features added: 'age' ('year' - 'birth_year') and 'years_climbed' ('year' - 'started').\n",
    "\n",
    "With the new 'age' and 'years_climbed' features created, the 'year', 'birth_year', and 'started' columns are no longer of interest and are dropped.\n",
    "\n",
    "Finally, to clarify some of the column names (and remain consistent), the following labels are renamed:\n",
    "* 'user_id' to 'id_user'\n",
    "* 'method_id' to 'id_method'\n",
    "* 'grade_id' to 'id_grade'\n",
    "* 'climb_type' to 'is_bouldering' $^{[a]}$\n",
    "\n",
    "-------------\n",
    "\n",
    "Notes\n",
    "\n",
    "$^{[a]}$ The 'climb_type' variable is binary and consists of 0s and 1s. By comparing the climbs with the most 0s and the most 1s (ie the most popular climbs for each 'climb_type') to their descriptions on another popular online log (www.mountainproject.com), it became clear that 'climb_type' = 0 refers to rope climbs and 'climb_type' = 1 refers to bouldering problems. As such, the variable was renamed to 'is_bouldering' to reduce any future confusion.\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that there are four different method types (redpoint, flash, onsight, and toprope). For the purposes of this analysis, toprope ascents are not considered successful $^{[a]}$ and so all ascents with an 'id_method' = 4 are dropped from the working dataframe. Then the 'id_method' column is dropped.\n",
    "\n",
    "At this point the working dataframe consists of ~4 million rows and 9 columns.\n",
    "\n",
    "--------------\n",
    "\n",
    "Notes:\n",
    "\n",
    "$^{[a]}$ Toproping is when a person climbs attached to the rop which goes up to an anchor at the top of the climb and then back down to the belayer. \n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 A note about NULL/bogus values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted here that, despite having cleaned up the data already, NULL and bogus values have been left intact on purpose. Specifically, the date data ('years_climbing' and 'age') is full of NULL values as well as values that simply don't make sense (negatives, 0s, and impossibly large values). Likewise, the weight data includes some 0s and the height data includes values ranging from 0cm tall to 255cm tall. \n",
    "\n",
    "All of this data is purposefully left in because to remove them all would drastically reduce the dataset even for variables that are otherwise fine. For example, over 21% of the observations have NULL values for 'age' (as a result of either the original 'year' or 'birth_year' being NULL). However, just because users chose not to input year data does not mean that the rest of their data is bad! As such, the entirety of the dataset is kept intact and the following filters will be applied as necessary:\n",
    "\n",
    "* 'height': 120-240 cm \n",
    "* 'weight': not 0 (equivalent to 40-100 kg)\n",
    "* 'age': 10-50\n",
    "* 'years_climbing': 0-40\n",
    "\n",
    "**Note that these filters are somewhat arbitrary!** There certainly may be climbers in the dataset shorter than 120 cm, taller than 240 cm, with more than 40 years of climbing experience, or out outside the age range of 10-50. However, it is assumed that most users that have these outlier vales are (a) either just that, outliers or (b) simply input incorrect data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accurately gage a climber's skill level, it is important not to consider every single grade ever climbed but instead to extract each person's personal best. Much like a world-class sprinter does not always run to beat a record, climbers very often - mostly, really - climb at grades that are below their historical high. As such, the total working dataframe is reduced to only include each user's *oldest* maximum 'id_grade' for both rope climbs and boulder problems.\n",
    "\n",
    "With the 'id_ascent' column no longer needed for such sorting and grouping, it is dropped.\n",
    "\n",
    "The head of the working dataframe is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_user</th>\n",
       "      <th>is_female</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>is_bouldering</th>\n",
       "      <th>id_grade</th>\n",
       "      <th>age</th>\n",
       "      <th>years_climbing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_user  is_female  height  weight  is_bouldering  id_grade   age  \\\n",
       "0        1          0     177      73              0        62  25.0   \n",
       "1        1          0     177      73              1        51  26.0   \n",
       "2        2          0       0       0              0        49   NaN   \n",
       "3        2          0       0       0              1        46   NaN   \n",
       "4        3          0     180      78              0        62  26.0   \n",
       "5        3          0     180      78              1        55  28.0   \n",
       "\n",
       "   years_climbing  \n",
       "0             5.0  \n",
       "1             6.0  \n",
       "2             1.0  \n",
       "3             1.0  \n",
       "4             4.0  \n",
       "5             6.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Separate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizing that rope climbing and bouldering are quite different despite both being considered \"rock climbing,\" it was decided to separate the dataset based on 'is_bouldering'. Similarly, the datasets are separated on 'is_female' as well to get different results based on gender. There are thus five different dataframes to be used depending on the analysis at hand:\n",
    "\n",
    "* df_all (49,598 rows, 8 columns)\n",
    "* df_bouldering_female (2,396 rows, 6 columns)\n",
    "* df_bouldering_male (16,817 rows, 6 columns)\n",
    "* df_rope_female (4,592 rows, 6 columns)\n",
    "* df_rope_male (25,793 rows, 6 columns)\n",
    "\n",
    "Note that the four data subsets contain six columns instead of eight like ```df_all```; this is because 'is_female' and 'is_bouldering' are not useful and are dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Exploratory data analysis\n",
    "It is always a good idea to do at least a bit of exploratory data analysis before diving too deep into an analysis; it can shed light on trends, potential problems, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pairplot of all of the non-dummy variables is shown below; successful projects are green while failed are blue. There does not seem to be good separation of *launch_state* values with the exception perhaps of *goal*: it does appear as if projects with large goals have few successes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pair_plot.jpeg\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 *funding_days* vs *goal*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows *funding_days* vs *goal*; the left plot shows all data points while the right is zoomed in on the *goal* (x-) axis to $0-$250,000. We can see that while there is no significant separation at these lower goal levels, there might be a slight benefit to having longer *funding_days*. Successful launches seem loosely clustered around *funding_days* = [0,60] and *goal* < $100k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='images/funding_days_vs_goal.jpeg'></td><td><img src='images/funding_days_vs_goal_2.jpeg'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 *staff_pick*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above in the [Variable - outcome correlation](#3.3.3) section, *staff_pick* has a relatively high correlation with the outcome of 0.25. Indeed, the images below show that projects chosen as a staff pick (*staff_pick* = 1) rarely fail to become successfully funded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='images/staff_pick_vs_launch_state.jpeg'></td><td><img src='images/staff_pick_vs_launch_state_2.jpeg'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plot below shows that the average *launch_state* for *staff_pick* of 0 and 1 is 0.523 and 0.889, respectively. Since *launch_state* is binary with 0 for failures and 1 for successes, these means also represent successful launch percentages, ie 52.3% of projects that are not chosen as a staff pick succeed while 88.9% of projects that are chosen succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/average_launch_state_vs_staff_pick.jpeg\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should also be noted that 13.5% of the projects were chosen as staff picks. From https://www.kickstarter.com/blog/how-to-get-featured-on-kickstarter, it appears as if projects are featured when they catch the eye of the Kickstarter staff via creativity, a nice and visually appealing site, etc. ie, they are NOT just picked due to them being funded well.\n",
    "\n",
    "Clearly, being chosen as a staff pick is correlated with funding success. What is unclear, however, is whether getting chosen actually helps in success or if projects that were already going to be successful are chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.4 Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 15 main categories that a project can fall under (with hundreds of sub-categories). A frequency plot of these 15 categories is shown below. The two most common categories are 'film & video' and 'music'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/category_freq_plot.jpeg\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above plot says nothing of the success of these categories. For that, I've plotted the average *launch_state* value (which, again, can be interpreted as the percentage of successes) as a function of category, below. The three most successful categories are 'comics', 'dance', and 'publishing' while the least successful are 'journalism', 'technology', and 'food'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/launch_state_vs_category_barplot.jpeg\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better show that some categories have better chances of success than other, I've plotted a heatmap and a clustermap, below. Successes (*launch_state* = 1) are white and failures (*luanch_state* = 0) are black. I used 0.5 for empty cells so as not to bias towards success or failure since a project can only be one category and so most values will indeed be empty. As such, in both figures, a mostly-white column can be interpreted as a highly successful category while a mostly-black column can be interpreted as not very successful. It can be clearly seen that, as indicated above, 'journalism' and 'technology' categories have a relatively high failure rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/category_heatmap.jpeg\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/category_clustermap.jpeg\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Model exploration\n",
    "\n",
    "Refer to appendix [A1.4 Model exploration](#A1.4) for code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start creating the machine learning models, I separated the working dataset into train and test sets. Note that using 25% of the working set as the test size ensures that 20% of the entire raw set is reserved for the test set (this is not exact since we did some cleaning after creating the validation set, but it's close). Also note that we dropped the outcome *launch_state* variable as well as several information-only variables from the dataset to be used for machine learning variables, X.\n",
    "\n",
    "```\n",
    "info_variables = ['id','launched_at','category','country', 'pledged_ratio', 'backers_count']\n",
    "\n",
    "X = df.drop(columns=info_variables).drop(columns='launch_state')\n",
    "y = df['launch_state']\n",
    "\n",
    "#-----------------------------------------\n",
    "# TRAIN/TEST SPLIT\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=101)\n",
    "```\n",
    "\n",
    "Next, the training and test datasets were scaled.\n",
    "\n",
    "```\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "```\n",
    "\n",
    "With the data scaled, I built and analyzed all of the models of interest by\n",
    "1. loading the relevant classifier.\n",
    "2. fitting the training data and outcome vector to the classifier.\n",
    "3. predicting the test outcomes using the trained classifier on the test data.\n",
    "4. printing the confusion matrix and the classification report comparing the known test outcomes to the predicted outcomes.\n",
    "5. calculating the one-run accuracy as the sum of the confusion matrix diagonal divided by the sum of the entire confusion matrix.\n",
    "6. completing a 10-fold cross validation for models that did not take too long to run.\n",
    "7. printing the 10-fold cross validation accuracies, mean accuracy, and accuracy standard deviation.\n",
    "\n",
    "Again, all code can be found in [A1.4 Model exploration](#A1.4).\n",
    "\n",
    "The results of these initial exploratory runs are shown in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>time_fit</th>\n",
       "      <th>time_predict</th>\n",
       "      <th>time_10_fold_CV</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>acc_10_fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.101767</td>\n",
       "      <td>0.036270</td>\n",
       "      <td>1.53579</td>\n",
       "      <td>0.623551</td>\n",
       "      <td>0.624725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.822522</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>17.0696</td>\n",
       "      <td>0.694333</td>\n",
       "      <td>0.689984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K Nearest Neighbors</td>\n",
       "      <td>7.340739</td>\n",
       "      <td>101.319590</td>\n",
       "      <td>344.234</td>\n",
       "      <td>0.682739</td>\n",
       "      <td>0.675807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM, Linear</td>\n",
       "      <td>954.375944</td>\n",
       "      <td>90.726687</td>\n",
       "      <td>None</td>\n",
       "      <td>0.677168</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM, RBF</td>\n",
       "      <td>1091.777649</td>\n",
       "      <td>120.427602</td>\n",
       "      <td>None</td>\n",
       "      <td>0.682908</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.437127</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>4.82765</td>\n",
       "      <td>0.664357</td>\n",
       "      <td>0.662544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (10-fold)</td>\n",
       "      <td>1.016743</td>\n",
       "      <td>0.104582</td>\n",
       "      <td>10.463</td>\n",
       "      <td>0.683248</td>\n",
       "      <td>0.680784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model     time_fit  time_predict time_10_fold_CV  \\\n",
       "0              Naive Bayes     0.101767      0.036270         1.53579   \n",
       "1      Logistic Regression     1.822522      0.009085         17.0696   \n",
       "2      K Nearest Neighbors     7.340739    101.319590         344.234   \n",
       "3              SVM, Linear   954.375944     90.726687            None   \n",
       "4                 SVM, RBF  1091.777649    120.427602            None   \n",
       "5            Decision Tree     0.437127      0.014542         4.82765   \n",
       "6  Random Forest (10-fold)     1.016743      0.104582          10.463   \n",
       "\n",
       "   accuracy acc_10_fold  \n",
       "0  0.623551    0.624725  \n",
       "1  0.694333    0.689984  \n",
       "2  0.682739    0.675807  \n",
       "3  0.677168        None  \n",
       "4  0.682908        None  \n",
       "5  0.664357    0.662544  \n",
       "6  0.683248    0.680784  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, all of the models seem to have similar accuracies (between ~62% and ~69%) while their run times vary greatly. \n",
    "\n",
    "A quick look into variable reduction via principal component analysis (PCA) was then completed. The code for this is found in Appendix [A1.4.9 Principal component analysis](#A1.4.9). Fitting the training data to a PCA object with only the top two principal components results in the following scatter plot. As expected (since there was not great separation when all of the variables were included), it does not seem like reducing the variables to their principal components make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pca_2components.jpeg\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, the PCA object with two principal components was fit to the Naive Bayes algorithm. It ran very fast (about 5x faster than the original Naive Bayes model) and head nearly the same accuracy (61% instead of 62%). This would be a great approach in some cases to speed things up while maintaining similar accuracy! The results table below now includes this new PCA result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>time_fit</th>\n",
       "      <th>time_predict</th>\n",
       "      <th>time_10_fold_CV</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>acc_10_fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.101767</td>\n",
       "      <td>0.036270</td>\n",
       "      <td>1.53579</td>\n",
       "      <td>0.623551</td>\n",
       "      <td>0.624725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.822522</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>17.0696</td>\n",
       "      <td>0.694333</td>\n",
       "      <td>0.689984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K Nearest Neighbors</td>\n",
       "      <td>7.340739</td>\n",
       "      <td>101.319590</td>\n",
       "      <td>344.234</td>\n",
       "      <td>0.682739</td>\n",
       "      <td>0.675807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM, Linear</td>\n",
       "      <td>954.375944</td>\n",
       "      <td>90.726687</td>\n",
       "      <td>None</td>\n",
       "      <td>0.677168</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM, RBF</td>\n",
       "      <td>1091.777649</td>\n",
       "      <td>120.427602</td>\n",
       "      <td>None</td>\n",
       "      <td>0.682908</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.437127</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>4.82765</td>\n",
       "      <td>0.664357</td>\n",
       "      <td>0.662544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (10-fold)</td>\n",
       "      <td>1.016743</td>\n",
       "      <td>0.104582</td>\n",
       "      <td>10.463</td>\n",
       "      <td>0.683248</td>\n",
       "      <td>0.680784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PCA (n=2), Naive Bayes</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.284533</td>\n",
       "      <td>0.609355</td>\n",
       "      <td>0.608927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model     time_fit  time_predict time_10_fold_CV  \\\n",
       "0              Naive Bayes     0.101767      0.036270         1.53579   \n",
       "1      Logistic Regression     1.822522      0.009085         17.0696   \n",
       "2      K Nearest Neighbors     7.340739    101.319590         344.234   \n",
       "3              SVM, Linear   954.375944     90.726687            None   \n",
       "4                 SVM, RBF  1091.777649    120.427602            None   \n",
       "5            Decision Tree     0.437127      0.014542         4.82765   \n",
       "6  Random Forest (10-fold)     1.016743      0.104582          10.463   \n",
       "7   PCA (n=2), Naive Bayes     0.024343      0.003595        0.284533   \n",
       "\n",
       "   accuracy acc_10_fold  \n",
       "0  0.623551    0.624725  \n",
       "1  0.694333    0.689984  \n",
       "2  0.682739    0.675807  \n",
       "3  0.677168        None  \n",
       "4  0.682908        None  \n",
       "5  0.664357    0.662544  \n",
       "6  0.683248    0.680784  \n",
       "7  0.609355    0.608927  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Model tuning and selection\n",
    "\n",
    "Refer to appendix [A1.5 Model tuning and selection](#A1.5) for code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it was time to choose some models to tune and make a final selection. This was completed for three models using sklearn's *GridSearchCV* method to hone in on the parameters that optimize the results. These optimal parameters are outlined below:\n",
    "\n",
    "* Random forest:\n",
    "    * *n_estimators* = 100\n",
    "    * *criterion* = \"gini\"\n",
    "    * *max_features* = \"sqrt\"\n",
    "    * *min_samples_leaf* = 25\n",
    "\n",
    "* Logistic regression:\n",
    "    * *C* = 10\n",
    "    * *penalty* = \"l1\"\n",
    "\n",
    "* K nearest neighbors:\n",
    "    * *metric* = \"minkowski\"\n",
    "    * *n_neighbors* = 23\n",
    "    * *p* = 2\n",
    "\n",
    "We also plotted validation curves for the numeric parameters to double-check for over-fitting. These curves are shown below (**Note: The image of the KNN validation curve was unortunately not saved before the entire session was pickled and dumped, ie I do not have that particular plot**). Note that they agree with the above outline; the models are made complicated enough to minimize variance but not so complicated that bias becomes unnecessarily large. Interestingly, the training and cross-validation curves do not start deviating from each other (high bias) when the random forest mode's *n_neighbors* is high which is what I would expect from an over-fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='images/validation_curves_random_forest.jpeg'></td><td><img src='images/validation_curves_logistic_regression.jpeg'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three models with optimized parameters were then run and results appended to the results table as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>time_fit</th>\n",
       "      <th>time_predict</th>\n",
       "      <th>time_10_fold_CV</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>acc_10_fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.101767</td>\n",
       "      <td>0.036270</td>\n",
       "      <td>1.53579</td>\n",
       "      <td>0.623551</td>\n",
       "      <td>0.624725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.822522</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>17.0696</td>\n",
       "      <td>0.694333</td>\n",
       "      <td>0.689984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K Nearest Neighbors</td>\n",
       "      <td>7.340739</td>\n",
       "      <td>101.319590</td>\n",
       "      <td>344.234</td>\n",
       "      <td>0.682739</td>\n",
       "      <td>0.675807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM, Linear</td>\n",
       "      <td>954.375944</td>\n",
       "      <td>90.726687</td>\n",
       "      <td>None</td>\n",
       "      <td>0.677168</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM, RBF</td>\n",
       "      <td>1091.777649</td>\n",
       "      <td>120.427602</td>\n",
       "      <td>None</td>\n",
       "      <td>0.682908</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.437127</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>4.82765</td>\n",
       "      <td>0.664357</td>\n",
       "      <td>0.662544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (10-fold)</td>\n",
       "      <td>1.016743</td>\n",
       "      <td>0.104582</td>\n",
       "      <td>10.463</td>\n",
       "      <td>0.683248</td>\n",
       "      <td>0.680784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PCA (n=2), Naive Bayes</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.284533</td>\n",
       "      <td>0.609355</td>\n",
       "      <td>0.608927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest (Optimized)</td>\n",
       "      <td>5.856856</td>\n",
       "      <td>0.470576</td>\n",
       "      <td>66.9746</td>\n",
       "      <td>0.718257</td>\n",
       "      <td>0.716067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression (Optimized)</td>\n",
       "      <td>6.454474</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>68.8582</td>\n",
       "      <td>0.695662</td>\n",
       "      <td>0.691059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>KNN (Optimized)</td>\n",
       "      <td>7.253727</td>\n",
       "      <td>48.397783</td>\n",
       "      <td>211.557</td>\n",
       "      <td>0.700328</td>\n",
       "      <td>0.69877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model     time_fit  time_predict  \\\n",
       "0                       Naive Bayes     0.101767      0.036270   \n",
       "1               Logistic Regression     1.822522      0.009085   \n",
       "2               K Nearest Neighbors     7.340739    101.319590   \n",
       "3                       SVM, Linear   954.375944     90.726687   \n",
       "4                          SVM, RBF  1091.777649    120.427602   \n",
       "5                     Decision Tree     0.437127      0.014542   \n",
       "6           Random Forest (10-fold)     1.016743      0.104582   \n",
       "7            PCA (n=2), Naive Bayes     0.024343      0.003595   \n",
       "8         Random Forest (Optimized)     5.856856      0.470576   \n",
       "9   Logistic Regression (Optimized)     6.454474      0.003956   \n",
       "10                  KNN (Optimized)     7.253727     48.397783   \n",
       "\n",
       "   time_10_fold_CV  accuracy acc_10_fold  \n",
       "0          1.53579  0.623551    0.624725  \n",
       "1          17.0696  0.694333    0.689984  \n",
       "2          344.234  0.682739    0.675807  \n",
       "3             None  0.677168        None  \n",
       "4             None  0.682908        None  \n",
       "5          4.82765  0.664357    0.662544  \n",
       "6           10.463  0.683248    0.680784  \n",
       "7         0.284533  0.609355    0.608927  \n",
       "8          66.9746  0.718257    0.716067  \n",
       "9          68.8582  0.695662    0.691059  \n",
       "10         211.557  0.700328     0.69877  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the prediction accuracy increases slightly for all three models when using optimized parameters. An optimized random forest model provides the best accuracy at ~72% while at the same time remaining efficient and fast to run. This is the chosen model for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Prediction / final validation\n",
    "\n",
    "Refer to appendix [A2 Code - new data prediction](#A2) for relevant code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With one specific model chosen to launch, it is time for a final validation. In this case, this validation also serves as a proving grounds for the final product. Recall that thus far we have not touched 20% of the initial raw data - this untouched set was set aside as a validation set. Keeping a completely separated validation set is useful because it ensures that the model in no way relied on it. This is in contrast to the training set which was used extensively to fit and train the model. Even the test set was used indirectly to baseline the model when doing accuracy comparisons. Only the validation set has been 100% unused.\n",
    "\n",
    "The goal here is to then create a script that takes in raw Kickstarter project data (again, in this case that raw data is the validation set previously set aside), cleans it up, and uses the chosen machine learning algorithm to predict which projects will be successfully funded or not. Since we do have the real-life outcomes of this set, we can also calculate our final product accuracy.\n",
    "\n",
    "Aside from some extra code that asks the user for input, runs various handling exceptions, and deals with whether or not the input file is a truly raw JSON file or whether it is an extracted dataframe from the JSON file (as is the case here with the validation set), the product script is quite simple. It \n",
    "\n",
    "1. takes in the raw data.\n",
    "2. cleans the data.\n",
    "3. extracts the relevant machine learning columns as well as the outcome column (if one exists).\n",
    "4. applies the prediction algorithm.\n",
    "5. writes out the prediction vector to a CSV file.\n",
    "6. evaluates the accuracy of the prediction vector to the known values, if applicable. Specifically, it prints the confusion matrix, classification report, and calculates the accuracy as the sum of the confusion matrix diagonal divided by the sum of the entire confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the script is quite simple. \n",
    "1. Ensure that Python is properly installed.\n",
    "2. Ensure that the following files are located in the working directly:\n",
    "\t* classifier_rf_opt.pkl\n",
    "\t* f_cleanData.py\n",
    "\t* f_dataImport.py\n",
    "\t* f_predict.py\n",
    "\t* predict.py\n",
    "\t* sc_X.pkl\n",
    "3. Create a folder named 'data' in the working directory.\n",
    "4. Download or create the Kickstarter data to run the prediction model on.\n",
    "\t* The data must be in JSON format like from https://webrobots.io/kickstarter-datasets/.\n",
    "\t* Alternatively, the data can a comma-separated value dataframe from previously-run analyses.\n",
    "5. Save the raw data in the 'data' folder.\n",
    "6. Open a command prompt.\n",
    "7. Type 'python predict.py'\n",
    "8. Follow the prompts.\n",
    "\n",
    "A screen shot of this process is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/validation_cmd.jpg\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above image also shows the confusion matrix, classification report, and calculated accuracy. As expected, **the model performs admirably with a 70% accuracy and managed to complete the analysis in only 1.49 seconds**. Further, a copy of the prediction vector was saved in the home folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Next steps\n",
    "\n",
    "Some recommendations to improve this analysis include:\n",
    "\n",
    "* Explore and account for any outliers that exist.\n",
    "* Use datetime information instead of simply extracting the years to get more accurate age and years_climbing data.\n",
    "* Explore the *rate* at which each user progresses.\n",
    "* User more data! Users of 8a.nu tend to be rather strong climbers and so this analysis leans very much towards those who have made rock climbing a significant part of their life. It therefore may not be appropriate to draw conclusons about more casual rock climbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
